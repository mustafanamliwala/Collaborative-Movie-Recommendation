{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EtHBk8LYz1Zv"
   },
   "source": [
    "### Implementing FunkSVD - Solution\n",
    "\n",
    "In this notebook we will take a look at writing our own function that performs FunkSVD, which will follow the steps you saw in the previous video.  If you find that you aren't ready to tackle this task on your own, feel free to skip to the following video where you can watch as I walk through the steps.\n",
    "\n",
    "To test our algorithm, we will run it on the subset of the data you worked with earlier.  Run the cell below to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wyGV_yN2z1Zw",
    "outputId": "c071eecf-ce2b-4b1f-f023-006e6e4b54b3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "# import svd_tests as t\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the datasets\n",
    "movies = pd.read_csv('movies.csv')\n",
    "reviews = pd.read_csv('ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId             title                                       genres\n",
       "0        1  Toy Story (1995)  Adventure|Animation|Children|Comedy|Fantasy\n",
       "1        2    Jumanji (1995)                   Adventure|Children|Fantasy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating   timestamp  Implicit\n",
      "0       1       16     4.0  1217897793         1\n",
      "1       1       24     1.5  1217895807         1\n",
      "\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "reviews['Implicit'] = 1\n",
    "implicit_matrix = reviews.pivot(index='userId', columns='movieId', values='Implicit')\n",
    "implicit_np = implicit_matrix.fillna(0).values\n",
    "print(reviews.head(2))\n",
    "print()\n",
    "print(implicit_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan ... nan nan nan]\n",
      " [5.  nan 2.  ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [3.  3.  2.  ... nan 4.5 nan]]\n"
     ]
    }
   ],
   "source": [
    "# Create user-by-item matrix\n",
    "user_items = reviews[['userId', 'movieId', 'rating', 'timestamp']]\n",
    "user_by_movie = user_items.groupby(['userId', 'movieId'])['rating'].first().unstack()\n",
    "# basically this line creating matrix with userID and Movie id and filling the values as rating\n",
    "\n",
    "# Create data subset\n",
    "ratings_mat = np.matrix(user_by_movie)\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1217897793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1217895807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1217896246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating   timestamp\n",
       "0       1       16     4.0  1217897793\n",
       "1       1       24     1.5  1217895807\n",
       "2       1       32     4.0  1217896246"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_items.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBbsGkQFz1Z0"
   },
   "source": [
    "`1.` You will use the **user_movie_subset** matrix to show that your FunkSVD algorithm will converge.  In the below cell, use the comments and document string to assist you as you complete writing your own function to complete FunkSVD.  You may also want to try to complete the funtion on your own without the assistance of comments.  You may feel free to remove and add to the function in any way that gets you a working solution! \n",
    "\n",
    "**Notice:** There isn't a sigma matrix in this version of matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0-HFYw-z1Z0"
   },
   "outputs": [],
   "source": [
    "def FunkSVD(ratings_mat, latent_features=4, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using a basic form of FunkSVD with no regularization\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values\n",
    "    latent_features - (int) the number of latent features used\n",
    "    learning_rate - (float) the learning rate \n",
    "    iters - (int) the number of iterations\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) a user by latent feature matrix\n",
    "    movie_mat - (numpy array) a latent feature by movie matrix\n",
    "    '''\n",
    "    \n",
    "    # Set up useful values to be used through the rest of the function\n",
    "    n_users = ratings_mat.shape[0]\n",
    "    n_movies = ratings_mat.shape[1]\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))  ## return the count non NAN values\n",
    "    \n",
    "    # initialize the user and movie matrices with random values\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    \n",
    "    # initialize sse at 0 for first iteration\n",
    "    sse_accum = 0\n",
    "    \n",
    "    # header for running results\n",
    "    print(\"Optimizaiton Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "    \n",
    "    # for each iteration\n",
    "    for iteration in range(iters):\n",
    "\n",
    "        # update our sse\n",
    "        old_sse = sse_accum\n",
    "        sse_accum = 0\n",
    "        \n",
    "        # For each user-movie pair\n",
    "        for i in range(n_users):\n",
    "            for j in range(n_movies):\n",
    "                \n",
    "                # if the rating exists\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    \n",
    "                    # compute the error as the actual minus the dot product of the user and movie latent features\n",
    "                    diff = ratings_mat[i, j] - np.dot(user_mat[i, :], movie_mat[:, j])\n",
    "                    \n",
    "                    # Keep track of the sum of squared errors for the matrix\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # update the values in each matrix in the direction of the gradient\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2*diff*movie_mat[k, j])\n",
    "                        movie_mat[k, j] += learning_rate * (2*diff*user_mat[i, k])\n",
    "\n",
    "        # print results for iteration\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / num_ratings))\n",
    "        \n",
    "    return user_mat, movie_mat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXhUwcRrz1Z3"
   },
   "source": [
    "`2.` Try out your function on the **user_movie_subset** dataset.  First try 4 latent features, a learning rate of 0.005, and 10 iterations.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW2uothLz1Z3",
    "outputId": "c49ee75b-8594-42c7-c85f-f85a10116dde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 1.818485\n",
      "2 \t\t 1.005561\n",
      "3 \t\t 0.815117\n",
      "4 \t\t 0.757099\n",
      "5 \t\t 0.726403\n",
      "6 \t\t 0.706726\n",
      "7 \t\t 0.692563\n",
      "8 \t\t 0.681494\n",
      "9 \t\t 0.672288\n",
      "10 \t\t 0.664250\n"
     ]
    }
   ],
   "source": [
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "677ssGAvz1Z5",
    "outputId": "e2355d7f-6648-4b1c-8515-7362ae2378df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.74254625 3.12564738 2.94960103 ... 2.95777575 3.7813217  3.54420361]\n",
      " [4.15346294 3.42347213 3.28718451 ... 3.16479875 4.23408375 4.01544629]\n",
      " [3.87773304 3.16788576 3.08569479 ... 2.95070402 3.86948398 3.55391836]\n",
      " ...\n",
      " [3.60165184 2.81919134 2.85003319 ... 2.44972091 3.58200693 3.35486406]\n",
      " [3.72761626 3.16716278 3.08553802 ... 3.00710869 4.05674581 3.91766681]\n",
      " [3.64610261 3.02690147 2.92221242 ... 2.83406739 3.75253126 3.53785642]]\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [5.  nan 2.  ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [3.  3.  2.  ... nan 4.5 nan]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzSUP8qMz1Z7"
   },
   "source": [
    "**The predicted ratings from the dot product are already starting to look a lot like the original data values even after only 10 iterations.  You can see some extreme low values that are not captured well yet.  The 5 in the second to last row in the first column is predicted as an 8, and the 4 in the second row and second column is predicted to be a 7.  Clearly the model is not done learning, but things are looking good.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdKyoyQyz1Z8"
   },
   "source": [
    "`3.` Let's try out the function again on the **user_movie_subset** dataset.  This time we will again use 4 latent features and a learning rate of 0.005.  However, let's bump up the number of iterations to 250.  When you take the dot product of the resulting U and V matrices, how does the resulting **user_movie** matrix compare to the original subset of the data?  What do you notice about your error at the end of the 250 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0dlcI91z1Z8",
    "outputId": "ee9ee25d-52b7-4abd-8d8e-b8a7fe652fc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizaiton Statistics\n",
      "Iterations | Mean Squared Error \n",
      "1 \t\t 1.805307\n",
      "2 \t\t 1.003988\n",
      "3 \t\t 0.813784\n",
      "4 \t\t 0.756192\n",
      "5 \t\t 0.725731\n",
      "6 \t\t 0.706158\n",
      "7 \t\t 0.691983\n",
      "8 \t\t 0.680800\n",
      "9 \t\t 0.671383\n",
      "10 \t\t 0.663040\n",
      "11 \t\t 0.655354\n",
      "12 \t\t 0.648065\n",
      "13 \t\t 0.641017\n",
      "14 \t\t 0.634127\n",
      "15 \t\t 0.627363\n",
      "16 \t\t 0.620726\n",
      "17 \t\t 0.614238\n",
      "18 \t\t 0.607926\n",
      "19 \t\t 0.601819\n",
      "20 \t\t 0.595936\n",
      "21 \t\t 0.590292\n",
      "22 \t\t 0.584892\n",
      "23 \t\t 0.579739\n",
      "24 \t\t 0.574828\n",
      "25 \t\t 0.570155\n"
     ]
    }
   ],
   "source": [
    "user_mat, movie_mat = FunkSVD(ratings_mat, latent_features=4, learning_rate=0.005, iters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jTRMW3upz1Z_",
    "outputId": "df43c2ae-8042-4279-c9eb-93f22722df86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.86369583 3.13803501 2.90827713 ... 2.74455902 4.35590931 3.54761509]\n",
      " [4.23847276 3.35930304 3.28389381 ... 2.91620664 4.93164154 4.47033271]\n",
      " [3.83463129 2.95393082 3.00763943 ... 2.55329904 4.50206533 4.24820136]\n",
      " ...\n",
      " [3.80665088 2.64934229 2.57000037 ... 2.24605603 4.08644329 3.10014865]\n",
      " [3.979836   3.11098902 3.02227418 ... 2.70033302 4.54507699 3.94050355]\n",
      " [3.60615454 2.69598211 2.68312209 ... 2.31610277 4.10468441 3.60656789]]\n",
      "[[nan nan nan ... nan nan nan]\n",
      " [5.  nan 2.  ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [3.  3.  2.  ... nan 4.5 nan]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(user_mat, movie_mat))\n",
    "print(ratings_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sq1w4Sf-z1aB"
   },
   "source": [
    "**In this case, we were able to completely reconstruct the item-movie matrix to obtain an essentially 0 mean squared error. I obtained 0 MSE on iteration 165.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuXlWJ7Fz1aB"
   },
   "source": [
    "The last time we placed an **np.nan** value into this matrix the entire svd algorithm in python broke.  Let's see if that is still the case using your FunkSVD function.  In the below cell, I have placed a nan into the first cell of your numpy array.  \n",
    "\n",
    "`4.` Use 4 latent features, a learning rate of 0.005, and 250 iterations.  Are you able to run your SVD without it breaking (something that was not true about the python built in)?  Do you get a prediction for the nan value?  What is your prediction for the missing value? Use the cells below to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDcrqtyUz1aC",
    "outputId": "17341084-3b8d-4f38-880e-406142a37b1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [5. , nan, 2. , ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [3. , 3. , 2. , ..., nan, 4.5, nan]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_mat[0, 0] = np.nan\n",
    "ratings_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AFO0rO7uz1aG",
    "outputId": "e18a37ec-d578-4799-e0c5-4f83bca40719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted value for the missing rating is : 4.238472760919372\n",
      "\n",
      "The actual value for the missing rating is : 5.0\n",
      "\n",
      "That's right! You just predicted a rating for a user-movie pair that was never rated!\n",
      "But if you look in the original matrix, this was actually a value of 10. Not bad!\n"
     ]
    }
   ],
   "source": [
    "preds = np.dot(user_mat, movie_mat)\n",
    "print(\"The predicted value for the missing rating is : {}\".format(preds[1,0]))\n",
    "print()\n",
    "print(\"The actual value for the missing rating is : {}\".format(ratings_mat[1,0]))\n",
    "print()\n",
    "assert np.isnan(preds[0,0]) == False\n",
    "print(\"That's right! You just predicted a rating for a user-movie pair that was never rated!\")\n",
    "print(\"But if you look in the original matrix, this was actually a value of 10. Not bad!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UyWMdzEz1aL"
   },
   "source": [
    "`6.` Now that you have a set of predictions for each user-movie pair.  Let's answer a few questions about your results. Provide the correct values to each of the variables below, and check your solutions using the tests below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_K0Khzoz1aL",
    "outputId": "a38b2274-1bc1-4dab-8b42-39aef07b4351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of actual ratings in the first_1000_users is 105339.\n",
      "\n",
      "The number of ratings made for user-movie pairs that didn't have ratings is 6791761\n"
     ]
    }
   ],
   "source": [
    "# How many actual ratings exist in first_1000_users\n",
    "num_ratings = np.count_nonzero(~np.isnan(first_1000_users))\n",
    "print(\"The number of actual ratings in the first_1000_users is {}.\".format(num_ratings))\n",
    "print()\n",
    "\n",
    "\n",
    "# How many ratings did we make for user-movie pairs that didn't have ratings\n",
    "ratings_for_missing = first_1000_users.shape[0]*first_1000_users.shape[1] - num_ratings\n",
    "print(\"The number of ratings made for user-movie pairs that didn't have ratings is {}\".format(ratings_for_missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcnfjArTz1aN",
    "outputId": "dd38a180-8209-47ca-f52a-61e525acd39a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\n"
     ]
    }
   ],
   "source": [
    "# Test your results against the solution\n",
    "assert num_ratings == 105339, \"Oops!  The number of actual ratings doesn't quite look right.\"\n",
    "assert ratings_for_missing == 6791761, \"Oops!  The number of movie-user pairs that you made ratings for that didn't actually have ratings doesn't look right.\"\n",
    "\n",
    "# Make sure you made predictions on all the missing user-movie pairs\n",
    "preds = np.dot(user_mat, movie_mat)\n",
    "assert np.isnan(preds).sum() == 0\n",
    "print(\"Nice job!  Looks like you have predictions made for all the missing user-movie pairs! But I still have one question... How good are they?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 10325)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 10325)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.86369583, 3.13803501, 2.90827713, ..., 2.74455902, 4.35590931,\n",
       "        3.54761509],\n",
       "       [4.23847276, 3.35930304, 3.28389381, ..., 2.91620664, 4.93164154,\n",
       "        4.47033271],\n",
       "       [3.83463129, 2.95393082, 3.00763943, ..., 2.55329904, 4.50206533,\n",
       "        4.24820136],\n",
       "       ...,\n",
       "       [3.80665088, 2.64934229, 2.57000037, ..., 2.24605603, 4.08644329,\n",
       "        3.10014865],\n",
       "       [3.979836  , 3.11098902, 3.02227418, ..., 2.70033302, 4.54507699,\n",
       "        3.94050355],\n",
       "       [3.60615454, 2.69598211, 2.68312209, ..., 2.31610277, 4.10468441,\n",
       "        3.60656789]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[nan, nan, nan, ..., nan, nan, nan],\n",
       "        [5. , nan, 2. , ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [nan, nan, nan, ..., nan, nan, nan],\n",
       "        [3. , 3. , 2. , ..., nan, 4.5, nan]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svdpp(ratings_mat, implicit, latent_features=4, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    SVD++ Matrix Factorization with biases and implicit feedback.\n",
    "    \n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) User-Item matrix with ratings.\n",
    "    implicit - (numpy array) User-Item implicit feedback matrix.\n",
    "    latent_features - (int) Number of latent features.\n",
    "    learning_rate - (float) Learning rate for gradient descent.\n",
    "    iters - (int) Number of iterations for gradient descent.\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_mat - (numpy array) User factors matrix.\n",
    "    movie_mat - (numpy array) Item factors matrix.\n",
    "    bu - (numpy array) User bias.\n",
    "    bi - (numpy array) Item bias.\n",
    "    '''\n",
    "    # Useful values\n",
    "    n_users, n_movies = ratings_mat.shape\n",
    "    num_ratings = np.count_nonzero(~np.isnan(ratings_mat))\n",
    "    \n",
    "    # Initialize matrices & biases\n",
    "    user_mat = np.random.rand(n_users, latent_features)\n",
    "    movie_mat = np.random.rand(latent_features, n_movies)\n",
    "    implicit_factors = np.random.rand(n_movies, latent_features)\n",
    "    mu = np.nanmean(ratings_mat)  # global bias\n",
    "    bu = np.random.rand(n_users)  # user biases\n",
    "    bi = np.random.rand(n_movies)  # item biases\n",
    "    \n",
    "    # Iterative optimization\n",
    "    for iteration in range(iters):\n",
    "        sse_accum = 0\n",
    "        for i in range(n_users):\n",
    "            N_u = np.where(implicit[i, :] == 1)[0]\n",
    "            N_u_len = np.sqrt(len(N_u) + 1e-10)  # adding a small value to avoid division by zero\n",
    "            for j in range(n_movies):\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    implicit_sum = np.sum(implicit_factors[N_u, :], axis=0)\n",
    "                    implicit_factor = implicit_sum / N_u_len\n",
    "                    \n",
    "                    # Calculate the error for this rating\n",
    "                    dot_product = np.dot(user_mat[i, :] + implicit_factor, movie_mat[:, j])\n",
    "                    pred = mu + bu[i] + bi[j] + dot_product\n",
    "                    diff = ratings_mat[i, j] - pred\n",
    "                    \n",
    "                    # Update biases\n",
    "                    bu[i] += learning_rate * (diff - 0.001 * bu[i])  # 0.001 is a regularization factor\n",
    "                    bi[j] += learning_rate * (diff - 0.001 * bi[j])  # same here\n",
    "                    \n",
    "                    # Update matrices\n",
    "                    for k in range(latent_features):\n",
    "                        user_mat[i, k] += learning_rate * (2 * diff * movie_mat[k, j] - 0.001 * user_mat[i, k])\n",
    "                        movie_mat[k, j] += learning_rate * (2 * diff * (user_mat[i, k] + implicit_factor[k]) - 0.001 * movie_mat[k, j])\n",
    "                        for l in N_u:\n",
    "                            implicit_factors[l, k] += learning_rate * (diff * movie_mat[k, j] / N_u_len - 0.001 * implicit_factors[l, k])\n",
    "                    \n",
    "                    # Sum squared error\n",
    "                    sse_accum += diff ** 2\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Iteration {iteration+1}: MSE = {sse_accum / num_ratings:.5f}\")\n",
    "        \n",
    "    return user_mat, movie_mat, bu, bi, implicit_factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mustu\\AppData\\Local\\Temp\\ipykernel_13316\\4034074066.py:55: RuntimeWarning: overflow encountered in double_scalars\n",
      "  movie_mat[k, j] += learning_rate * (2 * diff * (user_mat[i, k] + implicit_factor[k]) - 0.001 * movie_mat[k, j])\n",
      "C:\\Users\\mustu\\AppData\\Local\\Temp\\ipykernel_13316\\4034074066.py:60: RuntimeWarning: overflow encountered in double_scalars\n",
      "  sse_accum += diff ** 2\n",
      "C:\\Users\\mustu\\AppData\\Local\\Temp\\ipykernel_13316\\4034074066.py:55: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  movie_mat[k, j] += learning_rate * (2 * diff * (user_mat[i, k] + implicit_factor[k]) - 0.001 * movie_mat[k, j])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user_matsvd, movie_matsvd, ysvd \u001b[38;5;241m=\u001b[39m \u001b[43msvdpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratings_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimplicit_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36msvdpp\u001b[1;34m(ratings_mat, implicit, latent_features, learning_rate, iters)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_movies):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ratings_mat[i, j] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m         implicit_sum \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimplicit_factors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mN_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m         implicit_factor \u001b[38;5;241m=\u001b[39m implicit_sum \u001b[38;5;241m/\u001b[39m N_u_len\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# Calculate the error for this rating\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2256\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2260\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_matsvd, movie_matsvd, ysvd = svdpp(ratings_mat, implicit_np, latent_features=4, learning_rate=0.005, iters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_mat, movie_mat, bu, bi, mu, implicit_factors, user_idx, movie_idx, implicit_vec):\n",
    "    N_u = np.where(implicit_vec == 1)[0]\n",
    "    implicit_sum = np.sum(implicit_factors[N_u, :], axis=0)\n",
    "    implicit_factor = implicit_sum / np.sqrt(len(N_u) + 1e-10)  # avoid division by zero\n",
    "    dot_product = np.dot(user_mat[user_idx, :] + implicit_factor, movie_mat[:, movie_idx])\n",
    "    pred = mu + bu[user_idx] + bi[movie_idx] + dot_product\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_index = 5\n",
    "movie_index = 6\n",
    "predicted_rating = predict_rating(user_index, movie_index, U, M, Y,implicit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymmetric SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AsymmetricSVD(ratings_mat, user_implicit_mat, latent_features=4, learning_rate=0.0001, iters=100):\n",
    "    '''\n",
    "    This function performs matrix factorization using AsymmetricSVD for implicit feedback.\n",
    "\n",
    "    INPUT:\n",
    "    ratings_mat - (numpy array) a matrix with users as rows, movies as columns, and ratings as values.\n",
    "    user_implicit_mat - (numpy array) a matrix representing implicit interactions of users with items.\n",
    "    latent_features - (int) the number of latent features used.\n",
    "    learning_rate - (float) the learning rate.\n",
    "    iters - (int) the number of iterations.\n",
    "\n",
    "    OUTPUT:\n",
    "    q_mat - (numpy array) a user by latent feature matrix representing explicit preferences.\n",
    "    w_mat - (numpy array) a latent feature by item matrix for implicit interactions.\n",
    "    user_bias - (numpy array) bias for each user.\n",
    "    item_bias - (numpy array) bias for each item.\n",
    "    global_bias - (float) global bias.\n",
    "    '''\n",
    "    \n",
    "    n_users, n_items = ratings_mat.shape\n",
    "    global_bias = np.mean(ratings_mat[np.where(ratings_mat > 0)])\n",
    "    ratings_mat = np.array(ratings_mat)\n",
    "    \n",
    "    user_bias = np.zeros(n_users)\n",
    "    item_bias = np.zeros(n_items)\n",
    "    \n",
    "    q_mat = np.random.rand(n_items, latent_features)\n",
    "    w_mat = np.random.rand(latent_features, n_items)\n",
    "\n",
    "    print(\"Optimization Statistics\")\n",
    "    print(\"Iterations | Mean Squared Error \")\n",
    "\n",
    "    for iteration in range(iters):\n",
    "        sse_accum = 0\n",
    "        \n",
    "        for i in range(n_users):\n",
    "            for j in range(n_items):\n",
    "                if ratings_mat[i, j] > 0:\n",
    "                    N_u = np.where(user_implicit_mat[i, :] == 1)[0]\n",
    "                    implicit_sum = np.sum(q_mat[N_u, :] * ratings_mat[i, N_u][:, np.newaxis], axis=0)\n",
    "\n",
    "                    pred = global_bias + user_bias[i] + item_bias[j] + np.dot(implicit_sum, w_mat[:, j])\n",
    "                    diff = ratings_mat[i, j] - pred\n",
    "                    sse_accum += diff**2\n",
    "                    \n",
    "                    # Update biases\n",
    "                    user_bias[i] += learning_rate * (diff - 0.002 * user_bias[i])\n",
    "                    item_bias[j] += learning_rate * (diff - 0.002 * item_bias[j])\n",
    "\n",
    "                    # Update factors\n",
    "                    for k in range(latent_features):\n",
    "                        for l in N_u:\n",
    "                            q_mat[l, k] += learning_rate * (diff * w_mat[k, j] * ratings_mat[i, l] - 0.002 * q_mat[l, k])\n",
    "                            w_mat[k, j] += learning_rate * (diff * implicit_sum[k] - 0.002 * w_mat[k, j])\n",
    "\n",
    "        print(\"%d \\t\\t %f\" % (iteration+1, sse_accum / np.count_nonzero(~np.isnan(ratings_mat))))\n",
    "\n",
    "    return q_mat, w_mat, user_bias, item_bias, global_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Statistics\n",
      "Iterations | Mean Squared Error \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mustu\\AppData\\Local\\Temp\\ipykernel_18700\\2736632005.py:44: RuntimeWarning: overflow encountered in double_scalars\n",
      "  sse_accum += diff**2\n",
      "C:\\Users\\mustu\\AppData\\Local\\Temp\\ipykernel_18700\\2736632005.py:54: RuntimeWarning: overflow encountered in double_scalars\n",
      "  w_mat[k, j] += learning_rate * (diff * implicit_sum[k] - 0.002 * w_mat[k, j])\n",
      "C:\\Users\\mustu\\AppData\\Local\\Temp\\ipykernel_18700\\2736632005.py:54: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  w_mat[k, j] += learning_rate * (diff * implicit_sum[k] - 0.002 * w_mat[k, j])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m user_matsvd, movie_matsvd,ubias, ibias, bias \u001b[38;5;241m=\u001b[39m \u001b[43mAsymmetricSVD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratings_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimplicit_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [45]\u001b[0m, in \u001b[0;36mAsymmetricSVD\u001b[1;34m(ratings_mat, user_implicit_mat, latent_features, learning_rate, iters)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_items):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ratings_mat[i, j] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 39\u001b[0m         N_u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[43muser_implicit_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     40\u001b[0m         implicit_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(q_mat[N_u, :] \u001b[38;5;241m*\u001b[39m ratings_mat[i, N_u][:, np\u001b[38;5;241m.\u001b[39mnewaxis], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     42\u001b[0m         pred \u001b[38;5;241m=\u001b[39m global_bias \u001b[38;5;241m+\u001b[39m user_bias[i] \u001b[38;5;241m+\u001b[39m item_bias[j] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(implicit_sum, w_mat[:, j])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "user_matsvd, movie_matsvd,ubias, ibias, bias = AsymmetricSVD(ratings_mat, implicit_np, latent_features=4, learning_rate=0.005, iters=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both AsymmetricSVD and SVD++ are extensions of the traditional matrix factorization approach to incorporate implicit feedback. While they do seem similar due to the incorporation of implicit feedback, there are nuanced differences in how they handle and utilize this feedback. Here's a comparison:\n",
    "\n",
    "1. **Approach to Implicit Feedback**:\n",
    "    - **AsymmetricSVD**: Implicit feedback is combined directly with the user latent factors. Specifically, for a given user, the sum of the latent features of the items they have implicitly interacted with is added to the user's latent factors. This adjusted user factor is then used for predictions.\n",
    "    - **SVD++**: Implicit feedback is considered as an additional set of factors (usually termed `Y`). These are summed up and combined with the original user factors before making a prediction. In the case of SVD++, the implicit factors are more like \"corrections\" or \"adjustments\" to the user factors based on the items they've interacted with.\n",
    "\n",
    "2. **Model Complexity**:\n",
    "    - **AsymmetricSVD**: Typically simpler in terms of number of parameters, especially when considering a large number of items/users. The main addition is the implicit sum being added to user factors.\n",
    "    - **SVD++**: More parameters are introduced with the addition of the `Y` matrix. Given that every item has an associated vector in this `Y` matrix, the model can become computationally intensive for large datasets.\n",
    "\n",
    "3. **Predictions**:\n",
    "    - **AsymmetricSVD**: The prediction for a user-item pair is a dot product between the adjusted user factor (original user factors + sum of latent features from implicit interactions) and the item latent factors.\n",
    "    - **SVD++**: The prediction is a dot product between the adjusted user factor (original user factors + sum of `Y` values corresponding to items the user has interacted with) and the item latent factors.\n",
    "\n",
    "4. **Learning**:\n",
    "    - Both models update the latent factors using gradient descent. However, in SVD++, there's an additional step where the implicit factors (`Y` matrix) are updated based on the items the user has interacted with.\n",
    "\n",
    "5. **Practical Implications**:\n",
    "    - AsymmetricSVD might be faster to compute due to its lesser complexity. \n",
    "    - SVD++ can potentially capture more intricate patterns due to the additional parameters, but at the cost of computational complexity.\n",
    "\n",
    "To put it succinctly, while both methods aim to incorporate implicit feedback, they do so in slightly different ways. SVD++ introduces a distinct set of factors for the implicit feedback and adjusts user factors with these before making predictions. AsymmetricSVD, on the other hand, combines the user factors with a sum of item factors from the implicit feedback directly. These differences can lead to variations in prediction accuracy, training time, and computational complexity between the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_index, item_index, user_mat, item_mat, implicit_factors_mat, user_implicit_mat, user_bias, item_bias, global_bias):\n",
    "    '''\n",
    "    Predict the rating of a given user for a given item using the AsymmetricSVD model.\n",
    "\n",
    "    INPUT:\n",
    "    user_index - (int) Index of the user.\n",
    "    item_index - (int) Index of the item/movie.\n",
    "    user_mat - (numpy array) a user by latent feature matrix.\n",
    "    item_mat - (numpy array) a latent feature by movie matrix.\n",
    "    implicit_factors_mat - (numpy array) matrix of latent factors for implicit interactions.\n",
    "    user_implicit_mat - (numpy array) a matrix representing implicit interactions of users with items.\n",
    "    user_bias - (numpy array) bias for each user.\n",
    "    item_bias - (numpy array) bias for each item.\n",
    "    global_bias - (float) global bias.\n",
    "\n",
    "    OUTPUT:\n",
    "    prediction - (float) Predicted rating of the user for the item.\n",
    "    '''\n",
    "    \n",
    "    N_u = np.where(user_implicit_mat[user_index, :] == 1)[0]\n",
    "    implicit_sum = np.sum(implicit_factors_mat[N_u, :], axis=0)\n",
    "    user_pref = user_mat[user_index, :] + implicit_sum / np.sqrt(len(N_u))\n",
    "\n",
    "    prediction = global_bias + user_bias[user_index] + item_bias[item_index] + np.dot(user_pref, item_mat[item_index, :])\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example usage:\n",
    "# Let's say you want to predict the rating of user with index 5 for item with index 10:\n",
    "# predicted_rating = predict_rating(5, 10, user_mat, item_mat, implicit_factors_mat, user_implicit_mat, user_bias, item_bias, global_bias)\n",
    "# print(predicted_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Asymmetric SVD\n",
    "\n",
    "In traditional matrix factorization, we attempt to decompose the rating matrix \\( R \\) into two matrices. One matrix represents user latent factors, while the other represents item latent factors. The primary goal of this decomposition is to reconstruct the original matrix as closely as possible by the dot product of these two matrices.\n",
    "\n",
    "Asymmetric SVD offers a nuanced approach, especially with the incorporation of implicit feedback.\n",
    "\n",
    "Let's breakdown the structure of the predicted rating formula:\n",
    "\n",
    "$$\n",
    "\\tilde{r}_{ui} = \\mu + b_i + b_u + \\sum_{f=0}^{nfactors} \\sum_{j=0}^{nitems} r_{uj} Q_{j,f} W_{f,i}\n",
    "$$\n",
    "\n",
    "Here's what each component represents:\n",
    "\n",
    "1. \\( \\mu \\): This symbolizes the global bias or the average rating across all items and users. It establishes a general baseline for all ratings.\n",
    "\n",
    "2. \\( b_i \\): This is the bias for item \\( i \\). Essentially, it captures the divergence of ratings for item \\( i \\) from the global average.\n",
    "\n",
    "3. \\( b_u \\): This represents the bias for user \\( u \\), indicating how a user \\( u \\) rates items compared to the average.\n",
    "\n",
    "4. The double summation: This component is what distinctly characterizes Asymmetric SVD.\n",
    "   - \\( r_{uj} \\): Represents the rating user \\( u \\) allocates to item \\( j \\). \n",
    "   - \\( Q_{j,f} \\): Represents the weight of the \\( f^{th} \\) latent factor for item \\( j \\). Intriguingly, rather than presenting user preferences directly, this matrix discerns user preferences through their ratings.\n",
    "   - \\( W_{f,i} \\): Specifies the weight of the \\( f^{th} \\) latent factor for item \\( i \\).\n",
    "\n",
    "This double summation is fundamentally a weighted sum of all the latent factors for every item rated by user \\( u \\). This weighted sum is then employed to predict the rating for item \\( i \\).\n",
    "\n",
    "Furthermore, the equivalent item-item recommender would be expressed as:\n",
    "\n",
    "$$\n",
    "\\tilde{R} = RS = RQ^TW\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( R \\): The original matrix of ratings.\n",
    "- \\( Q^T \\): The transposed form of the Q matrix, which is pivotal in capturing the \"preferences\" based on ratings.\n",
    "- \\( W \\): This matrix is key in capturing the weight of latent factors for items.\n",
    "\n",
    "The product \\( RQ^T \\) results in a user-by-latent factor matrix. These latent factors are directly inferred from user ratings. Multiplying this with \\( W \\) produces the predicted rating matrix.\n",
    "\n",
    "The term \"asymmetric\" is derived from the fact that the similarity matrix, which arises from \\( Q \\) and \\( W \\), is inherently asymmetric. This asymmetry is attributed to the unique aspects captured by \\( Q \\) and \\( W \\). While \\( Q \\) deduces user preferences from their ratings, \\( W \\) discerns the significance of the latent factors for the items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2_Implementing_FunkSVD_Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
